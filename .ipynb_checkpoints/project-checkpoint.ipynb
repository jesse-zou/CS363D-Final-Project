{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "## Group Members: Jesse Zou, Andy Li, Yuhan Zheng, Zhiyao Bao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the data science problem you are trying to solve?\n",
    "    * We're trying to predict the trend of the stock market (if the stock market's price will go up or go down in general for the next day) for DOW30, SP500 and NASDAQ respectively.\n",
    "* Why does the problem matter?\n",
    "    * First, we will be able to have an unbiased analysis on the market and the economy. The stock market is influenced by human emotions and irrational sentiments. By being able to predict micro-movements in the market prices, we are able to also model human behavior. Additionally, it may help people involved in day trading to earn money. \n",
    "* What could the results of your predictive model be used for?\n",
    "    * The model can be used by people who trade in the stock market to make better decisions and potentially generate more profit on their investments.\n",
    "* Why would we want to be able to predict the thing you’re trying to predict?\n",
    "    * We want to be able to predict the stock market prices because it helps us to better understand the stock market.\n",
    "* Then describe the dataset that you will use to tackle this problem\n",
    "    * The dataset contains a data point for each day of the SP500, DOW30, and NASDAQ. These are stock indexes, that is, a sort of average for the entire stock market. By doing an analysis on this dataset, we are capturing the entire market and economy as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "#warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first cache the data sets here for continuous usage, taking away overhead that would happen from loading in the data set at each step of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data_dow = pd.read_csv(\"DOW30.csv\")\n",
    "data_sp = pd.read_csv(\"SP500.csv\")\n",
    "data_nas = pd.read_csv(\"NASDAQ.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing data cleaning, we first verify whether or not it is even necessary. Some data sets come fully intact without inconsistencies, so we check if the data sets contain any null values before continuing on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do not need data cleaning\n"
     ]
    }
   ],
   "source": [
    "def need_data_cleaning():\n",
    "    if data_dow.isnull().values.any() or data_sp.isnull().values.any() or data_nas.isnull().values.any():\n",
    "        print(\"has None value in dataset\")\n",
    "    else:\n",
    "        print(\"do not need data cleaning\")\n",
    "\n",
    "need_data_cleaning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got \"do not need data cleaning\", which means there is no \"Null\" value in any of those three datasets we are going to use (DOW30.csv, SP500.csv, and NASDAQ.csv). In this case, we can conclude that our datasets are good and no further data cleaning is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was then explored by using a box plot and a line graph. \n",
    "\n",
    "Using the box plot, the data set can be summarized into its minimum, 1st quartile, median, 3rd quartile, and maximum, along with any outliers that are contained in the data.\n",
    "\n",
    "Using the line graph, the data is visualized in a way that may help determine trends that are not as easily detected using machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ''",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e08a454bab82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflierprops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mred_square\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mbox_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mline_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "def box_plot(x):\n",
    "    red_square = dict(markerfacecolor='r', marker='s')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Horizontal Boxes')\n",
    "    ax.boxplot(x, vert=False, flierprops=red_square)\n",
    "\n",
    "box_plot(data_dow[\"\"])\n",
    "\n",
    "def line_graph(x, y):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM model performs 3 things:\n",
    "    \n",
    "    - Scales the data so that it is normalized to reduce runtime.\n",
    "    - Performs dimensionality reduction using PCA to reduce runtime.\n",
    "    - Runs SVM.\n",
    "Using these steps, we can mitigate the time spent training the SVM model without severely impacting the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "def SVM_trainer(data_X, data_Y):\n",
    "    svm_scaler = StandardScaler()\n",
    "    svm_pca = PCA()\n",
    "    svm = SVC()\n",
    "\n",
    "    svm_ppl = Pipeline(steps=[('scaler', svm_scaler), ('pca', svm_pca), ('svm', svm)])\n",
    "\n",
    "    svm_param_grid = {\n",
    "        'pca__n_components': list(range(1, 11)),\n",
    "        'svm__kernel': ['linear', 'rbf', 'poly']\n",
    "    }\n",
    "\n",
    "    svm_grid_search = GridSearchCV(svm_ppl, svm_param_grid, cv=5, scoring='accuracy')\n",
    "#     svm_scores = cross_val_score(svm_grid_search, data_X, data_Y, cv=10)\n",
    "#     svm_preds = cross_val_predict(svm_grid_search, data_X, data_Y, cv=10)\n",
    "#     print(\"Accuracy:\", svm_scores.mean()*100, \"%\")\n",
    "#     print(\"classification report:\\n\",classification_report(data_Y, svm_preds))\n",
    "    return svm_grid_search\n",
    "\n",
    "# SVM_trainer(data_X, data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "def KNN_trainer(data_X, data_Y):\n",
    "    scaler = StandardScaler()\n",
    "    pca = PCA()\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=7)\n",
    "    ppl = Pipeline(steps=[('scaler', scaler), ('pca', pca), ('knn', knn_classifier)])\n",
    "#     scores = cross_val_score(ppl, data_X, data_Y, cv=5) \n",
    "#     print(\"Accuracy:\", scores.mean()*100, \"%\")\n",
    "\n",
    "    param_grid = {\n",
    "        'pca__n_components': list(range(1, 11)),\n",
    "        'knn__n_neighbors': list(range(1, 26))\n",
    "    }\n",
    "\n",
    "    knn_grid_search = GridSearchCV(ppl, param_grid, cv=5, scoring='accuracy')\n",
    "#     knn_grid_search.fit(data_X, data_Y)\n",
    "#     print(\"Best parameters:\", knn_grid_search.best_params_)\n",
    "#     print(\"Best score:\", knn_grid_search.best_score_*100, \"%\")\n",
    "\n",
    "#     knn_nested_score = cross_val_score(knn_grid_search, data_X, data_Y, cv=5)\n",
    "#     print(\"Accuracy:\", knn_nested_score.mean()*100, \"%\")\n",
    "    return knn_grid_search\n",
    "# knn_grid_search = KNN_trainer(data_X, data_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural Network model performs 3 things:\n",
    "    \n",
    "    - Scales the data so that it is normalized to reduce runtime and to prevent some features from outweighing others.\n",
    "    - Tests varying hidden layers in order to best determine which would have the greatest performance.\n",
    "    - Runs Neural Networks.\n",
    "Using these steps, we can mitigate the time spent training the Neural Network model without severely impacting the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def NN_trainer(data_X, data_Y):\n",
    "    nn_scaler = StandardScaler()\n",
    "    nn = MLPClassifier()\n",
    "\n",
    "    nn_ppl = Pipeline(steps=[('scaler', nn_scaler), ('nn', nn)])\n",
    "    nn_param_grid = {\n",
    "        'nn__hidden_layer_sizes': list(range(30, 61, 10)),\n",
    "        'nn__activation': ['logistic', 'tanh', 'relu']\n",
    "    }\n",
    "    nn_grid_search = GridSearchCV(nn_ppl, nn_param_grid, cv=5, scoring='accuracy')\n",
    "#     nn_scores = cross_val_score(nn_grid_search, data_X, data_Y, cv=5)\n",
    "#     print(\"Accuracy:\", nn_scores.mean()*100, \"%\")\n",
    "    return nn_grid_search\n",
    "\n",
    "# nn_grid_search = NN_trainer(nn_ppl, nn_param_grid, data_X, data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensamble\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def ensamble_trainer(svm_grid_search, knn_grid_search, nn_grid_search, data_X, data_Y):\n",
    "    eclf = VotingClassifier(\n",
    "        estimators=[('svm', svm_grid_search), ('knn', knn_grid_search), ('nn', nn_grid_search)],\n",
    "        voting='hard')\n",
    "    for clf, label in zip([svm_grid_search, knn_grid_search, nn_grid_search, eclf], ['SVM', 'KNN', 'Neural Network', 'Ensemble']):\n",
    "        scores = cross_val_score(clf, data_X, data_Y, scoring='accuracy', cv=5)\n",
    "        print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "        \n",
    "# ensamble_trainer(svm_grid_search, knn_grid_search, nn_grid_search, eclf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results with Various Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried three different feature engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: without tracking the prev day; no date, TEDSpread, EFFR\n",
    "* In the first one, we removed the date since we want to see whether we can get good prediction results assuming each day’s stock price is independent from the previous days. We also removed TEDSpred and EFFR features since initially we believe they are not quite related to the prediction of the stock price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOW30\n",
    "data_dow_processed = data_dow.drop(['Date', 'TEDSpread', 'EFFR'],axis=1)\n",
    "# data_dow_processed.head()\n",
    "data_dow_Y = data_dow_processed['LABEL']\n",
    "data_dow_X = data_dow_processed.drop(['LABEL'],axis=1)\n",
    "svm_grid_search = SVM_trainer(data_dow_X, data_dow_Y)\n",
    "knn_grid_search = KNN_trainer(data_dow_X, data_dow_Y)\n",
    "nn_grid_search = NN_trainer(data_dow_X, data_dow_Y)\n",
    "ensamble_trainer(svm_grid_search, knn_grid_search, nn_grid_search, data_dow_X, data_dow_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "    Accuracy: 0.54 (+/- 0.00) [SVM]\n",
    "    Accuracy: 0.50 (+/- 0.03) [KNN]\n",
    "    Accuracy: 0.52 (+/- 0.04) [Neural Network]\n",
    "    Accuracy: 0.53 (+/- 0.03) [Ensemble]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SP500\n",
    "data_sp_processed = data_sp.drop(['Date', 'TEDSpread', 'EFFR'],axis=1)\n",
    "data_sp_Y = data_sp_processed['LABEL']\n",
    "data_sp_X = data_sp_processed.drop(['LABEL'],axis=1)\n",
    "svm_grid_search = SVM_trainer(data_sp_X, data_sp_Y)\n",
    "knn_grid_search = KNN_trainer(data_sp_X, data_sp_Y)\n",
    "nn_grid_search = NN_trainer(data_sp_X, data_sp_Y)\n",
    "ensamble_trainer(svm_grid_search, knn_grid_search, nn_grid_search, data_sp_X, data_sp_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "    Accuracy: 0.53 (+/- 0.04) [SVM]\n",
    "    Accuracy: 0.50 (+/- 0.02) [KNN]\n",
    "    Accuracy: 0.53 (+/- 0.03) [Neural Network]\n",
    "    Accuracy: 0.51 (+/- 0.04) [Ensemble]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASDAQ\n",
    "data_nas_processed = data_nas.drop(['Date', 'TEDSpread', 'EFFR'],axis=1)\n",
    "data_nas_Y = data_nas_processed['LABEL']\n",
    "data_nas_X = data_nas_processed.drop(['LABEL'],axis=1)\n",
    "SVM_trainer(data_nas_X, data_nas_Y)\n",
    "svm_grid_search = SVM_trainer(data_nas_X, data_nas_Y)\n",
    "knn_grid_search = KNN_trainer(data_nas_X, data_nas_Y)\n",
    "nn_grid_search = NN_trainer(data_nas_X, data_nas_Y)\n",
    "ensamble_trainer(svm_grid_search, knn_grid_search, nn_grid_search, data_nas_X, data_nas_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "    Accuracy: 0.56 (+/- 0.00) [SVM]\n",
    "    Accuracy: 0.47 (+/- 0.04) [KNN]\n",
    "    Accuracy: 0.56 (+/- 0.00) [Neural Network]\n",
    "    Accuracy: 0.55 (+/- 0.00) [Ensemble]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: tracking the prev day; no date, TEDSpread, EFFR\n",
    "* In the second one, we added the predictions of the previous day of the other two datasets, and we removed the date, TEDSpred, and EFFR as we did in the first. Comparing the first and the second, we are able to conclude if adding the predictions of the previous day of the other two datasets would improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "def process_data(target_dataset, dataset_label1, dataset_label2, label1, label2):\n",
    "    data_processed = target_dataset.drop(['Date', 'TEDSpread', 'EFFR'],axis=1)\n",
    "    labels1 = dataset_label1.iloc[0:, 1]\n",
    "    labels2 = dataset_label2.iloc[0:, 1]\n",
    "    data_processed[label1] = labels1\n",
    "    data_processed[label1] = data_processed[label1].shift(periods=1, fill_value=-1)\n",
    "    data_processed[label2] = labels2\n",
    "    data_processed[label2] = data_processed[label2].shift(periods=1, fill_value=-1)\n",
    "    data_processed = data_processed.iloc[1: , :]\n",
    "    return data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOW30\n",
    "data_dow_processed = process_data(data_dow, data_sp, data_nas, \"SP500\", \"NASDAQ\")\n",
    "# print(data_dow_processed.head())\n",
    "data_dow_Y = data_dow_processed['LABEL']\n",
    "data_dow_X = data_dow_processed.drop(['LABEL'],axis=1)\n",
    "svm_grid_search = SVM_trainer(data_dow_X, data_dow_Y)\n",
    "knn_grid_search = KNN_trainer(data_dow_X, data_dow_Y)\n",
    "nn_grid_search = NN_trainer(data_dow_X, data_dow_Y)\n",
    "ensamble_trainer(svm_grid_search, knn_grid_search, nn_grid_search, data_dow_X, data_dow_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "    Accuracy: 0.53 (+/- 0.01) [SVM]\n",
    "    Accuracy: 0.51 (+/- 0.03) [KNN]\n",
    "    Accuracy: 0.52 (+/- 0.03) [Neural Network]\n",
    "    Accuracy: 0.53 (+/- 0.02) [Ensemble]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SP500\n",
    "data_sp_processed = process_data(data_sp, data_dow, data_nas, \"DOW30\", \"NASDAQ\")\n",
    "data_sp_Y = data_sp_processed['LABEL']\n",
    "data_sp_X = data_sp_processed.drop(['LABEL'],axis=1)\n",
    "svm_grid_search = SVM_trainer(data_sp_X, data_sp_Y)\n",
    "knn_grid_search = KNN_trainer(data_sp_X, data_sp_Y)\n",
    "nn_grid_search = NN_trainer(data_sp_X, data_sp_Y)\n",
    "ensamble_trainer(svm_grid_search, knn_grid_search, nn_grid_search, data_sp_X, data_sp_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "    Accuracy: 0.55 (+/- 0.00) [SVM]\n",
    "    Accuracy: 0.50 (+/- 0.02) [KNN]\n",
    "    Accuracy: 0.54 (+/- 0.01) [Neural Network]\n",
    "    Accuracy: 0.54 (+/- 0.01) [Ensemble]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASDAQ\n",
    "data_nas_processed = process_data(data_nas, data_dow, data_sp, \"DOW30\", \"SP500\")\n",
    "data_nas_Y = data_nas_processed['LABEL']\n",
    "data_nas_X = data_nas_processed.drop(['LABEL'],axis=1)\n",
    "SVM_trainer(data_nas_X, data_nas_Y)\n",
    "svm_grid_search = SVM_trainer(data_nas_X, data_nas_Y)\n",
    "knn_grid_search = KNN_trainer(data_nas_X, data_nas_Y)\n",
    "nn_grid_search = NN_trainer(data_nas_X, data_nas_Y)\n",
    "ensamble_trainer(svm_grid_search, knn_grid_search, nn_grid_search, data_nas_X, data_nas_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "    Accuracy: 0.55 (+/- 0.02) [SVM]\n",
    "    Accuracy: 0.50 (+/- 0.04) [KNN]\n",
    "    Accuracy: 0.56 (+/- 0.00) [Neural Network]\n",
    "    Accuracy: 0.54 (+/- 0.02) [Ensemble]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: tracking the prev day; no date\n",
    "* In the third one, we added the predictions of the previous day of the other two datasets, and we only removed the date. Comparing the second and third, we are able to conclude if adding TEDSpred, and EFFR features would improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "def process_data_2(target_dataset, dataset_label1, dataset_label2, label1, label2):\n",
    "    data_processed = target_dataset.drop(['Date'],axis=1)\n",
    "    labels1 = dataset_label1.iloc[0:, 1]\n",
    "    labels2 = dataset_label2.iloc[0:, 1]\n",
    "    data_processed[label1] = labels1\n",
    "    data_processed[label1] = data_processed[label1].shift(periods=1, fill_value=-1)\n",
    "    data_processed[label2] = labels2\n",
    "    data_processed[label2] = data_processed[label2].shift(periods=1, fill_value=-1)\n",
    "    data_processed = data_processed.iloc[1: , :]\n",
    "    return data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOW30\n",
    "data_dow_processed = process_data_2(data_dow, data_sp, data_nas, \"SP500\", \"NASDAQ\")\n",
    "# print(data_dow_processed.head())\n",
    "data_dow_Y = data_dow_processed['LABEL']\n",
    "data_dow_X = data_dow_processed.drop(['LABEL'],axis=1)\n",
    "svm_grid_search = SVM_trainer(data_dow_X, data_dow_Y)\n",
    "knn_grid_search = KNN_trainer(data_dow_X, data_dow_Y)\n",
    "nn_grid_search = NN_trainer(data_dow_X, data_dow_Y)\n",
    "ensamble_trainer(svm_grid_search, knn_grid_search, nn_grid_search, data_dow_X, data_dow_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "    Accuracy: 0.54 (+/- 0.01) [SVM]\n",
    "    Accuracy: 0.50 (+/- 0.03) [KNN]\n",
    "    Accuracy: 0.53 (+/- 0.03) [Neural Network]\n",
    "    Accuracy: 0.53 (+/- 0.02) [Ensemble]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SP500\n",
    "data_sp_processed = process_data_2(data_sp, data_dow, data_nas, \"DOW30\", \"NASDAQ\")\n",
    "data_sp_Y = data_sp_processed['LABEL']\n",
    "data_sp_X = data_sp_processed.drop(['LABEL'],axis=1)\n",
    "svm_grid_search = SVM_trainer(data_sp_X, data_sp_Y)\n",
    "knn_grid_search = KNN_trainer(data_sp_X, data_sp_Y)\n",
    "nn_grid_search = NN_trainer(data_sp_X, data_sp_Y)\n",
    "ensamble_trainer(svm_grid_search, knn_grid_search, nn_grid_search, data_sp_X, data_sp_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "    Accuracy: 0.55 (+/- 0.00) [SVM]\n",
    "    Accuracy: 0.50 (+/- 0.02) [KNN]\n",
    "    Accuracy: 0.53 (+/- 0.03) [Neural Network]\n",
    "    Accuracy: 0.53 (+/- 0.02) [Ensemble]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASDAQ\n",
    "data_nas_processed = process_data_2(data_nas, data_dow, data_sp, \"DOW30\", \"SP500\")\n",
    "data_nas_Y = data_nas_processed['LABEL']\n",
    "data_nas_X = data_nas_processed.drop(['LABEL'],axis=1)\n",
    "SVM_trainer(data_nas_X, data_nas_Y)\n",
    "svm_grid_search = SVM_trainer(data_nas_X, data_nas_Y)\n",
    "knn_grid_search = KNN_trainer(data_nas_X, data_nas_Y)\n",
    "nn_grid_search = NN_trainer(data_nas_X, data_nas_Y)\n",
    "ensamble_trainer(svm_grid_search, knn_grid_search, nn_grid_search, data_nas_X, data_nas_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "    Accuracy: 0.55 (+/- 0.02) [SVM]\n",
    "    Accuracy: 0.48 (+/- 0.05) [KNN]\n",
    "    Accuracy: 0.55 (+/- 0.01) [Neural Network]\n",
    "    Accuracy: 0.54 (+/- 0.02) [Ensemble]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30295c5bec572e859485b1ffa5e89b8b3e2022ef6e3e739c1ac40f143a557caf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
